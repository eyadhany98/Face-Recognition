# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ckAEeBRtBiyyVG2HK3al36rseUkvaAm3
"""

#ALL imports
import os
import cv2
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

"""Data Preparation"""

#temp part for data extraction
from zipfile import ZipFile
fname = 'att-database-of-faces.zip'
with ZipFile(fname,'r') as zip:
  zip.extractall()
  print("done")

#data preparation 
D = [] #declare Data matrix D
y = [] #declare Labels matrix y
current_directory = os.getcwd() #get the current working directory as it contains the files of the data
for i in range(1,41):     #Loop over the 40 folders of the 40 people
  subFolder = os.listdir(current_directory + '/s' + str(i)) #open the folder containg the 10 images
  for image_number in subFolder: #Loop over the 10 images of each person
    img = cv2.imread(current_directory + '/s' + str(i) + '/' + image_number , 0) #read the image 
    image_vector = img.ravel() #reshape the 112 by 92 image to 1D array of size 112*92
    D.append(image_vector) #Add the photo vector to the data matrix
    y.append(int(i)) #Add the label of each photo to the label matrix
D = np.array(D) #convert the Data matrix from list to numpy array
y = np.array(y) #convert the labels matrix from list to numpy array
#splitting the data 
train = D[1::2,:]  #pick the even values for the training data by choosing every other row 
test = D[0::2,:]  #pick the odd values for the training data by choosing every other row
y_train = y[1::2] #pick the even values for the training data by choosing every other row
y_test = y[0::2]  #pick the odd values for the training data by choosing every other row

# RUN THIS for splitting the data using random splits with a ratio of 7:3
from sklearn.model_selection import train_test_split
train, test, y_train, y_test = train_test_split(D, y, test_size=0.30)

"""PCA"""

#PCA till getting eigen values and vectors
mean = np.mean(train, axis=0) 
centered = train - mean 
cov = (1/len(train)) * (centered.transpose().dot(centered))
eigen_values,eigen_vectors = np.linalg.eigh(cov)

#PCA from projection till 
alpha = np.array([0.8,0.85,0.9,0.95]) #tolerance values given
neighbors = np.array([1,3,5,7]) #K values for KNN
accuracies = [] #array to hold the values of the accuracies
for i in range(np.shape(alpha)[0]): #loop over the tolerance values
  indices = [] #array to store the indeces of the chosen eigen vector columns to be added to the basis
  index = np.shape(eigen_vectors)[1] - 1 #start the indeces from the largest eigen value which resides at the end 
  fraction = -1 #initialize the fraction by any negative value
  fraction_numerator = 0 #initialize the fraction numerator by zero
  while fraction < alpha[i]: #Loop as long as the fraction of total variance is less than the tolerance 
    indices.append(index) #include the eigen vector column index 
    fraction_numerator += eigen_values[index] #add the value of the corrosponding eigen value to the numerator to compute the fraction of variance
    fraction = fraction_numerator / np.sum(eigen_values) #compute the value of the fraction of total variance
    index -= 1 #decrement the index by one
  U = eigen_vectors[:,indices] #form the projection matrix U by selecting only the columns with indices picked by the loop
  train_projected = train.dot(U) #project the training data
  test_projected = test.dot(U)   #project the test data
  accuracy = [] #temp array to hold the accuracy values for each value of alpha
  for neighbor in range(0,4):
    classifier = KNeighborsClassifier(n_neighbors=neighbors[neighbor]) #declare a KNN classifier with K=1
    classifier.fit(train_projected, y_train) #place the trainig data with its labels
    y_pred = classifier.predict(test_projected) #perform the KNN prediction 
    accuracy.append(accuracy_score(y_test, y_pred))  #calculate the accuracy by comparing each predicted value to its actual value 
  accuracy = np.array(accuracy) #convert the list to numpy array
  accuracies.append(accuracy) #add the values for the accuracy to the accuracies array
accuracies = np.array(accuracies)  #convert the list to numpy array
plt.plot(alpha,accuracies.T[0]) #plot alpha against the accuracy
plt.title('accuracy against alpha for K=1')
plt.xlabel('alpha')
plt.ylabel('accuracy')
plt.show()
plt.plot(neighbors,accuracies.T) #plot K against the accuracy
legend = ['alpha=0.8','alpha=0.85' ,'alpha=0.9' , 'alpha=0.95'] 
plt.legend(legend)
plt.title('accuracy against K')
plt.xlabel('K')
plt.ylabel('accuracy')
plt.show()
print(accuracies)

"""LDA"""

#LDA
means = [] #array to hold the means of each of the classes
for i in range(1,41): #loop over the 40 classes
  means.append(np.mean(train[y_train==i], axis=0)) #calculate the mean of each separate class and add it to the means matrix 
means = np.array(means) #convert the list to numpy array
overall_mean = np.mean(train , axis=0) #calculate the overall mean
Sb = 0 #Between classes scatter matrix
for j in range(0,40): #loop over the 40 classes
  n = train[y_train==j+1,:].shape[0] #get the count of elements in each class
  mean_diff = means[j] - overall_mean #calculate ui-u
  Sb += n * mean_diff.reshape(10304,1).dot(mean_diff.reshape(1,10304)) #calculate matrix Sb by summing the values for each class(each iteration)
Sb = np.array(Sb) #convert the list to numpy array

S = 0 #scatter matrix
for k in range(0,40): #Loop over the 40 classes
  z = train[y_train==k+1,:] - means[k] #center the data
  S += z.T.dot(z) #calculate matrix S by summing the scatter matrices of each class

Sinv = np.linalg.pinv(S,hermitian = True) #compute the inverse of matrix S
eigen_vals,eigen_vects = np.linalg.eig(Sinv.dot(Sb)) #get the eigen vectors and values of Sinv times Sb

sorted_indeces = np.argsort(eigen_vals) [::-1] #sort the eigen values in descending order
eigen_vects = eigen_vects[:,sorted_indeces] #reshape the eigen vectors accordingly
u = eigen_vects[:,0:39] #pick the first 39 eigen vectors
train_proj = train.dot(u) #project the training data
test_proj = test.dot(u)   #project the test data
train_proj = train_proj.real #get the real part only
test_proj = test_proj.real #get the real part only

accuraciesLDA = [] #array to hold the values of the accuracies
for neighbor in range(0,4): #loop over the values of K for KNN
  classifier = KNeighborsClassifier(n_neighbors=neighbors[neighbor]) #declare a KNN classifier with K=1
  classifier.fit(train_proj, y_train) #place the trainig data with its labels
  y_pred = classifier.predict(test_proj) #perform the KNN prediction 
  accuraciesLDA.append(accuracy_score(y_test, y_pred)) #calculate the accuracy by comparing each predicted value to its actual value 
accuraciesLDA = np.array(accuraciesLDA) #convert the list to numpy array
plt.plot(neighbors,accuraciesLDA) #plot K against the accuracy
plt.title('accuracy against K')
plt.xlabel('K')
plt.ylabel('accuracy')
plt.show()
print(accuraciesLDA)